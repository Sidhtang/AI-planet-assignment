{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSPggtkUPAM1XhET5V3oEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/AI-planet-assignment/blob/main/Untitled49.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR7EWECsjbQ0",
        "outputId": "8a0a2ad4-0a80-4801-fc59-1db261d5b937"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.12.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTKAEAdzjTV8",
        "outputId": "98320272-0153-4bbe-f342-87de918e4add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 294,912 || all params: 109,809,210 || trainable%: 0.2686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/40: 100%|██████████| 5/5 [00:22<00:00,  4.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Loss: 17.207433700561523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/40: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/40, Loss: 16.400941848754883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/40: 100%|██████████| 5/5 [00:23<00:00,  4.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/40, Loss: 16.274045944213867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/40: 100%|██████████| 5/5 [00:19<00:00,  3.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/40, Loss: 15.446812629699707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/40: 100%|██████████| 5/5 [00:20<00:00,  4.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/40, Loss: 15.069193840026855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/40: 100%|██████████| 5/5 [00:20<00:00,  4.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/40, Loss: 14.406350135803223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/40: 100%|██████████| 5/5 [00:20<00:00,  4.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/40, Loss: 14.630973815917969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/40: 100%|██████████| 5/5 [00:20<00:00,  4.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/40, Loss: 13.498257637023926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/40: 100%|██████████| 5/5 [00:19<00:00,  3.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/40, Loss: 13.517868041992188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/40: 100%|██████████| 5/5 [00:20<00:00,  4.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/40, Loss: 12.602889060974121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/40: 100%|██████████| 5/5 [00:19<00:00,  4.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/40, Loss: 12.164481163024902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/40: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/40, Loss: 11.62293529510498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/40: 100%|██████████| 5/5 [00:20<00:00,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/40, Loss: 11.133036613464355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/40: 100%|██████████| 5/5 [00:19<00:00,  3.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/40, Loss: 10.022181510925293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/40: 100%|██████████| 5/5 [00:20<00:00,  4.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/40, Loss: 9.793307304382324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/40: 100%|██████████| 5/5 [00:20<00:00,  4.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/40, Loss: 9.121143341064453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/40: 100%|██████████| 5/5 [00:20<00:00,  4.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/40, Loss: 8.874080657958984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/40: 100%|██████████| 5/5 [00:20<00:00,  4.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/40, Loss: 8.535558700561523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/40: 100%|██████████| 5/5 [00:19<00:00,  3.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/40, Loss: 7.890665531158447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/40: 100%|██████████| 5/5 [00:20<00:00,  4.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/40, Loss: 7.828741550445557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/40: 100%|██████████| 5/5 [00:20<00:00,  4.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/40, Loss: 7.638747692108154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/40: 100%|██████████| 5/5 [00:19<00:00,  3.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/40, Loss: 7.146599292755127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/40: 100%|██████████| 5/5 [00:20<00:00,  4.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/40, Loss: 6.984192371368408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/40: 100%|██████████| 5/5 [00:20<00:00,  4.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/40, Loss: 6.9877142906188965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/40: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/40, Loss: 6.513589382171631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/40: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/40, Loss: 6.304955005645752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/40: 100%|██████████| 5/5 [00:19<00:00,  3.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/40, Loss: 6.049112319946289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/40: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/40, Loss: 6.088864803314209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/40: 100%|██████████| 5/5 [00:20<00:00,  4.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/40, Loss: 5.739612102508545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/40: 100%|██████████| 5/5 [00:19<00:00,  3.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/40, Loss: 5.610071659088135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/40: 100%|██████████| 5/5 [00:20<00:00,  4.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/40, Loss: 5.491029739379883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/40: 100%|██████████| 5/5 [00:19<00:00,  3.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/40, Loss: 5.3699164390563965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/40: 100%|██████████| 5/5 [00:21<00:00,  4.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/40, Loss: 5.305877208709717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/40: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/40, Loss: 5.019252777099609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/40: 100%|██████████| 5/5 [00:19<00:00,  3.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/40, Loss: 5.044049263000488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/40: 100%|██████████| 5/5 [00:20<00:00,  4.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/40, Loss: 4.842686176300049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/40: 100%|██████████| 5/5 [00:19<00:00,  3.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/40, Loss: 4.7420573234558105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/40: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/40, Loss: 4.74804162979126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/40: 100%|██████████| 5/5 [00:20<00:00,  4.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/40, Loss: 4.6828999519348145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/40: 100%|██████████| 5/5 [00:19<00:00,  3.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/40, Loss: 4.692572116851807\n",
            "Input: Leopard print faux fur coat with black leather belt, ideal for glamorous nights out.\n",
            "Generated tags: leopard print faux fur coat with black leather belt, ideal for glamorous nights out. residualgueghanysisyasyasyasyasyasysisysisyasyasyasyasyasyasyasyasyasyasyasysisyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas\n",
            "Latency: 112.9380 seconds\n",
            "\n",
            "Input: Teal blue wetsuit with neon yellow accents, perfect for surfing.\n",
            "Generated tags: teal blue wetsuit with neon yellow accents, perfect for surfing. kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan kellan\n",
            "Latency: 115.6613 seconds\n",
            "\n",
            "Input: Pale pink tutu with crystal embellishments, suitable for ballet performances.\n",
            "Generated tags: pale pink tutu with crystal embellishments, suitable for ballet performances... kimberleyyasyasyasyasyasyasyasletteyasyasyasyasyasyasyasyasyasyas kimberleyyasyasyasyasyasyasyas..luxluxluxyasyasyasyasyasyasyasyasyasyasluxyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas.luxyasyasyasluxyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas\n",
            "Latency: 115.5067 seconds\n",
            "\n",
            "Input: Olive green military-style jacket with brass buttons, great for casual outings.\n",
            "Generated tags: olive green military - style jacket with brass buttons, great for casual outings.earsyasearsyas.yasyasearsyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasedingyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas.yasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas\n",
            "Latency: 115.7505 seconds\n",
            "\n",
            "Input: Bright orange safety vest with reflective strips, essential for construction work.\n",
            "Generated tags: bright orange safety vest with reflective strips, essential for construction work.ilefixed. ▪tensgentminatinggentgentgentgentgentgentgentvanigentyasgentgentgenttersgentgentgentgentgentgentgentgentyasgentiledyasiredgentiredgentgentgentgentgentgentgentgentgentiredgentgentiredgentgentyasgentgentgentiredgentiredirediredirediredgentirediredirediredyasirediredyasirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredirediredired\n",
            "Latency: 115.8300 seconds\n",
            "\n",
            "Input: White lace wedding jumpsuit with wide legs, perfect for modern brides.\n",
            "Generated tags: white lace wedding jumpsuit with wide legs, perfect for modern brides.yasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasysisyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas\n",
            "Latency: 115.8658 seconds\n",
            "\n",
            "Input: Navy blue pin-dot suspenders, ideal for adding flair to formal wear.\n",
            "Generated tags: navy blue pin - dot suspenders, ideal for adding flair to formal wear.yasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyasyas\n",
            "Latency: 115.5911 seconds\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('outfit_tagger_model/tokenizer_config.json',\n",
              " 'outfit_tagger_model/special_tokens_map.json',\n",
              " 'outfit_tagger_model/vocab.txt',\n",
              " 'outfit_tagger_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, BertConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# In a real scenario, you would generate this data using GPT-4\n",
        "# Add these to the existing train_data list\n",
        "# Step 1: Prepare the data\n",
        "# In a real scenario, you would generate this data using GPT-4\n",
        "# Add these to the existing train_data list\n",
        "train_data = [\n",
        "    (\"Emerald green velvet blazer with gold buttons, perfect for holiday parties.\", \"1. Emerald Green\\n2. Velvet\\n3. Blazer\\n4. Gold Accents\\n5. Holiday Wear\"),\n",
        "    (\"Light blue denim overalls with distressed knees, great for casual weekends.\", \"1. Light Blue\\n2. Denim\\n3. Overalls\\n4. Distressed\\n5. Weekend Casual\"),\n",
        "    (\"Cream colored cable knit sweater, ideal for cozy winter days.\", \"1. Cream\\n2. Cable Knit\\n3. Sweater\\n4. Cozy\\n5. Winter Wear\"),\n",
        "    (\"Bright yellow raincoat with white polka dots, perfect for rainy spring days.\", \"1. Yellow\\n2. Polka Dot\\n3. Raincoat\\n4. Spring Wear\\n5. Weather Protection\"),\n",
        "    (\"Navy blue and white striped sailor shirt, great for beach outings.\", \"1. Navy Blue\\n2. Striped\\n3. Sailor Shirt\\n4. Beach Wear\\n5. Nautical Style\"),\n",
        "    (\"Burgundy leather ankle boots with chunky heels, suitable for fall fashion.\", \"1. Burgundy\\n2. Leather\\n3. Ankle Boots\\n4. Chunky Heel\\n5. Fall Fashion\"),\n",
        "    (\"Pastel pink tulle skirt with sequin waistband, ideal for ballet recitals.\", \"1. Pastel Pink\\n2. Tulle\\n3. Skirt\\n4. Sequin Detail\\n5. Performance Wear\"),\n",
        "    (\"Charcoal gray wool peacoat with large black buttons, perfect for winter in the city.\", \"1. Charcoal Gray\\n2. Wool\\n3. Peacoat\\n4. Winter Wear\\n5. Urban Style\"),\n",
        "    (\"Neon orange running shorts with built-in compression liner, great for marathons.\", \"1. Neon Orange\\n2. Running Shorts\\n3. Compression\\n4. Athletic Wear\\n5. Marathon Gear\"),\n",
        "    (\"Lavender satin slip dress with lace trim, perfect for romantic dinner dates.\", \"1. Lavender\\n2. Satin\\n3. Slip Dress\\n4. Lace Trim\\n5. Date Night\"),\n",
        "    (\"Olive green cargo pants with multiple pockets, ideal for hiking or outdoor activities.\", \"1. Olive Green\\n2. Cargo Pants\\n3. Multi-Pocket\\n4. Hiking Gear\\n5. Outdoor Wear\"),\n",
        "    (\"Black and gold sequin cocktail dress, great for New Year's Eve parties.\", \"1. Black and Gold\\n2. Sequin\\n3. Cocktail Dress\\n4. Party Wear\\n5. New Year's Eve\"),\n",
        "    (\"White cotton polo shirt with navy blue trim, perfect for golf outings.\", \"1. White\\n2. Cotton\\n3. Polo Shirt\\n4. Navy Trim\\n5. Golf Wear\"),\n",
        "    (\"Bright red patent leather pumps with pointed toe, ideal for making a bold statement.\", \"1. Bright Red\\n2. Patent Leather\\n3. Pumps\\n4. Pointed Toe\\n5. Statement Piece\"),\n",
        "    (\"Mustard yellow corduroy overalls, great for artsy, bohemian looks.\", \"1. Mustard Yellow\\n2. Corduroy\\n3. Overalls\\n4. Artsy\\n5. Bohemian Style\"),\n",
        "    (\"Pale blue chiffon maxi dress with floral print, perfect for garden weddings.\", \"1. Pale Blue\\n2. Chiffon\\n3. Maxi Dress\\n4. Floral Print\\n5. Wedding Guest\"),\n",
        "(\"Heather gray sweatpants with elastic ankles, ideal for lounging at home.\", \"1. Heather Gray\\n2. Sweatpants\\n3. Elastic Ankles\\n4. Loungewear\\n5. Comfortable\"),\n",
        "    (\"Black leather motorcycle jacket with silver studs, great for rock concerts.\", \"1. Black\\n2. Leather\\n3. Motorcycle Jacket\\n4. Silver Studs\\n5. Concert Wear\"),\n",
        "    (\"Turquoise bikini with gold chain straps, perfect for beach vacations.\", \"1. Turquoise\\n2. Bikini\\n3. Gold Accents\\n4. Swimwear\\n5. Beachwear\"),]\n",
        "\n",
        "class OutfitDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.data[idx][0]\n",
        "        target_text = self.data[idx][1]\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        targets = self.tokenizer.encode_plus(\n",
        "            target_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
        "            'target_ids': torch.tensor(targets['input_ids'], dtype=torch.long),\n",
        "            'target_attention_mask': torch.tensor(targets['attention_mask'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Step 2: Set up the model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Configure LoRA\n",
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, # Changed to TaskType.CAUSAL_LM\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"query\", \"value\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Step 3: Prepare the dataset\n",
        "dataset = OutfitDataset(train_data, tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Step 4: Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        target_ids = batch['target_ids'].to(device)\n",
        "        target_attention_mask = batch['target_attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Step 5: Inference function\n",
        "def generate_tags(input_text):\n",
        "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        # Added max_new_tokens to specify the number of tokens to generate\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=128, num_return_sequences=1)\n",
        "    end_time = time.time()\n",
        "\n",
        "    generated_tags = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    return generated_tags, latency\n",
        "\n",
        "# Step 6: Evaluation\n",
        "test_inputs = [\n",
        "    \"Leopard print faux fur coat with black leather belt, ideal for glamorous nights out.\",\n",
        "    \"Teal blue wetsuit with neon yellow accents, perfect for surfing.\",\n",
        "    \"Pale pink tutu with crystal embellishments, suitable for ballet performances.\",\n",
        "    \"Olive green military-style jacket with brass buttons, great for casual outings.\",\n",
        "    \"Bright orange safety vest with reflective strips, essential for construction work.\",\n",
        "    \"White lace wedding jumpsuit with wide legs, perfect for modern brides.\",\n",
        "    \"Navy blue pin-dot suspenders, ideal for adding flair to formal wear.\",\n",
        "\n",
        "]\n",
        "for test_input in test_inputs:\n",
        "    generated_tags, latency = generate_tags(test_input)\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"Generated tags: {generated_tags}\")\n",
        "    print(f\"Latency: {latency:.4f} seconds\")\n",
        "    print()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"outfit_tagger_model\")\n",
        "tokenizer.save_pretrained(\"outfit_tagger_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w53DK68bjbCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRTdR8IfjVIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvVx8nzyjVLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Wtpm5PtjVOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vtsL1wNijVRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "auolUMPBjVXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VD2FvFBLjVaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}